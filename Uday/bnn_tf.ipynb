{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+XmnomZdHwgEDCa93Wwf4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AWorldOfChaos/SoC-2024-Robust-ML/blob/main/Uday/bnn_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ykFS_mp8Z59",
        "outputId": "f72f55c5-d4af-4209-f7cb-591b9bc75e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1688/1688 [==============================] - 21s 8ms/step - loss: 1.3357 - accuracy: 0.7820 - val_loss: 0.6225 - val_accuracy: 0.8935\n",
            "Epoch 2/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.8387 - accuracy: 0.8645 - val_loss: 0.4756 - val_accuracy: 0.9325\n",
            "Epoch 3/100\n",
            "1688/1688 [==============================] - 15s 9ms/step - loss: 0.7394 - accuracy: 0.8772 - val_loss: 0.4457 - val_accuracy: 0.9237\n",
            "Epoch 4/100\n",
            "1688/1688 [==============================] - 14s 8ms/step - loss: 0.6298 - accuracy: 0.8871 - val_loss: 0.3959 - val_accuracy: 0.9378\n",
            "Epoch 5/100\n",
            "1688/1688 [==============================] - 17s 10ms/step - loss: 0.5207 - accuracy: 0.9011 - val_loss: 0.3220 - val_accuracy: 0.9412\n",
            "Epoch 6/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4665 - accuracy: 0.9104 - val_loss: 0.2522 - val_accuracy: 0.9530\n",
            "Epoch 7/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4165 - accuracy: 0.9205 - val_loss: 0.2811 - val_accuracy: 0.9523\n",
            "Epoch 8/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4087 - accuracy: 0.9224 - val_loss: 0.3031 - val_accuracy: 0.9497\n",
            "Epoch 9/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3953 - accuracy: 0.9263 - val_loss: 0.2903 - val_accuracy: 0.9503\n",
            "Epoch 10/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3499 - accuracy: 0.9319 - val_loss: 0.2463 - val_accuracy: 0.9628\n",
            "Epoch 11/100\n",
            "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3158 - accuracy: 0.9380 - val_loss: 0.2130 - val_accuracy: 0.9627\n",
            "Epoch 12/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3014 - accuracy: 0.9408 - val_loss: 0.1903 - val_accuracy: 0.9657\n",
            "Epoch 13/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2744 - accuracy: 0.9439 - val_loss: 0.2410 - val_accuracy: 0.9618\n",
            "Epoch 14/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2938 - accuracy: 0.9426 - val_loss: 0.2196 - val_accuracy: 0.9640\n",
            "Epoch 15/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2884 - accuracy: 0.9443 - val_loss: 0.2766 - val_accuracy: 0.9427\n",
            "Epoch 16/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2620 - accuracy: 0.9484 - val_loss: 0.2210 - val_accuracy: 0.9658\n",
            "Epoch 17/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2421 - accuracy: 0.9524 - val_loss: 0.1681 - val_accuracy: 0.9710\n",
            "Epoch 18/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2377 - accuracy: 0.9539 - val_loss: 0.1986 - val_accuracy: 0.9682\n",
            "Epoch 19/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2380 - accuracy: 0.9539 - val_loss: 0.1700 - val_accuracy: 0.9735\n",
            "Epoch 20/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2217 - accuracy: 0.9566 - val_loss: 0.1756 - val_accuracy: 0.9697\n",
            "Epoch 21/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2311 - accuracy: 0.9558 - val_loss: 0.1731 - val_accuracy: 0.9723\n",
            "Epoch 22/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2246 - accuracy: 0.9571 - val_loss: 0.1950 - val_accuracy: 0.9705\n",
            "Epoch 23/100\n",
            "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2258 - accuracy: 0.9573 - val_loss: 0.1969 - val_accuracy: 0.9703\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1945 - accuracy: 0.9680\n",
            "Test accuracy: 0.9679999947547913\n",
            "Model: \"bnn_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         multiple                  0         \n",
            "                                                                 \n",
            " binarized_dense_6 (Binariz  multiple                  401920    \n",
            " edDense)                                                        \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  multiple                  2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " binarized_dense_7 (Binariz  multiple                  131328    \n",
            " edDense)                                                        \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  multiple                  1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " binarized_dense_8 (Binariz  multiple                  32896     \n",
            " edDense)                                                        \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  multiple                  512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " binarized_dense_9 (Binariz  multiple                  8256      \n",
            " edDense)                                                        \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  multiple                  256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " binarized_dense_10 (Binari  multiple                  2080      \n",
            " zedDense)                                                       \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  multiple                  128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " binarized_dense_11 (Binari  multiple                  330       \n",
            " zedDense)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 580778 (2.22 MB)\n",
            "Trainable params: 578794 (2.21 MB)\n",
            "Non-trainable params: 1984 (7.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Flatten, BatchNormalization\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "@tf.custom_gradient\n",
        "def binarize(x):\n",
        "    def grad(dy):\n",
        "        return dy * tf.cast(tf.abs(x) <= 1, dtype=tf.float32)\n",
        "    return tf.where(x >= 0, 1.0, -1.0), grad\n",
        "\n",
        "# def hard_tanh(x):\n",
        "#     return tf.clip_by_value(x, -1, 1)\n",
        "\n",
        "class BinarizedDense(Layer):\n",
        "    def __init__(self, units, activation=None):\n",
        "        super(BinarizedDense, self).__init__()\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        binary_w = binarize(self.w)\n",
        "        outputs = tf.matmul(inputs, binary_w) + self.b\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "class BNNModel(Model):\n",
        "    def __init__(self):\n",
        "        super(BNNModel, self).__init__()\n",
        "        self.flatten = Flatten(input_shape=(28, 28))\n",
        "        self.dense1 = BinarizedDense(512)\n",
        "        self.bn1 = BatchNormalization()\n",
        "        self.dense2 = BinarizedDense(256)\n",
        "        self.bn2 = BatchNormalization()\n",
        "        self.dense3 = BinarizedDense(128)\n",
        "        self.bn3 = BatchNormalization()\n",
        "        self.dense4 = BinarizedDense(64)\n",
        "        self.bn4 = BatchNormalization()\n",
        "        self.dense5 = BinarizedDense(32)\n",
        "        self.bn5 = BatchNormalization()\n",
        "        self.dense6 = BinarizedDense(10, activation=tf.nn.softmax)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.flatten(inputs)\n",
        "        x = self.bn1(self.dense1(x))\n",
        "        x = binarize(x)\n",
        "        x = self.bn2(self.dense2(x))\n",
        "        # x = hard_tanh(x)\n",
        "        x = binarize(x)\n",
        "        x = self.bn3(self.dense3(x))\n",
        "        x = binarize(x)\n",
        "        x = self.bn4(self.dense4(x))\n",
        "        x = binarize(x)\n",
        "        x = self.bn5(self.dense5(x))\n",
        "        x = binarize(x)\n",
        "        return self.dense6(x)\n",
        "\n",
        "model = BNNModel()\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100, validation_split=0.1, callbacks=[early_stopping])\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "model.summary()\n"
      ]
    }
  ]
}